#!/usr/bin/env python3
# filename: wizard_bot.py
import os, asyncio, logging, time, io, re
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from PIL import Image
from telegram import Update
from telegram.error import BadRequest
from telegram.ext import (
    ApplicationBuilder, ContextTypes,
    CommandHandler, MessageHandler, filters, CallbackContext
)
import google.generativeai as genai
from flask import Flask, render_template_string # <-- –î–æ–±–∞–≤–ª—è–µ–º render_template_string
import threading # <-- –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç threading

# –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ Flask-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞
flask_app = Flask(__name__)

# HTML-—à–∞–±–ª–æ–Ω –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
HTML_TEMPLATE = """
<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>–ë–æ—Ç —Å–∏–≥–º–æ–∏–¥–∞ –∑–∞–ø—É—â–µ–Ω!</title>
    <style>
        body {
            background-color: #1a1a1a; /* –¢–µ–º–Ω–∞—è —Ç–µ–º–∞ */
            color: #f0f0f0; /* –°–≤–µ—Ç–ª—ã–π —Ç–µ–∫—Å—Ç */
            font-family: Arial, sans-serif;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            padding: 20px;
            box-sizing: border-box;
            text-align: center;
        }
        .container {
            background-color: #2c2c2c;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
            max-width: 600px;
            width: 100%;
        }
        h1 {
            color: #90ee90; /* –°–≤–µ—Ç–ª–æ-–∑–µ–ª–µ–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ */
            margin-bottom: 20px;
        }
        p {
            font-size: 1.1em;
            margin-bottom: 30px;
        }
        .tenor-gif-embed {
            margin: 20px auto; /* –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –≥–∏—Ñ–∫—É */
            max-width: 100%;
            height: auto;
        }
        .button-link {
            display: inline-block;
            background-color: #6a5acd; /* –§–∏–æ–ª–µ—Ç–æ–≤–∞—è –∫–Ω–æ–ø–∫–∞ */
            color: white;
            padding: 12px 25px;
            border-radius: 8px;
            text-decoration: none;
            font-size: 1.1em;
            transition: background-color 0.3s ease;
        }
        .button-link:hover {
            background-color: #7b68ee;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ü•≥ –ë–æ—Ç —Å–∏–≥–º–æ–∏–¥–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω!</h1>
        <p>–í–∞—à Telegram –±–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –≥–æ—Ç–æ–≤ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã.</p>
        <div class="tenor-gif-embed" data-postid="13327394754582742145" data-share-method="host" data-aspect-ratio="1" data-width="100%">
            <a href="https://tenor.com/view/cologne-wear-i-buddy-home-gif-13327394754582742145">Cologne Wear GIF</a>
            from <a href="https://tenor.com/search/cologne-gifs">Cologne GIFs</a>
        </div>
        <script type="text/javascript" async src="https://tenor.com/embed.js"></script>
        <br>
        <a href="https://xn--80aqtedp.xn--p1ai/" target="_blank" class="button-link">–°–∞–π—Ç —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π</a>
    </div>
</body>
</html>
"""


@flask_app.route('/')
def home():
    return render_template_string(HTML_TEMPLATE)

# Gemini API –∫–æ–Ω—Ñ–∏–≥
API_KEYS = []
for i in [1, 2]:
    key = os.getenv(f"GEMINI_API_KEY_{i}")
    if key:
        API_KEYS.append(key)

if not API_KEYS:
    raise RuntimeError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è GEMINI_API_KEY_1 –∏–ª–∏ GEMINI_API_KEY_2")
# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é (–æ—Ç –ª—É—á—à–µ–π –∫ —Ö—É–¥—à–µ–π)
MODELS = [
    "gemini-2.5-pro",
    "gemini-2.5-flash",
    "gemini-2.5-flash-preview",
    "gemini-2.5-flash-lite",
    "gemini-2.5-flash-lite-preview",
    "gemini-2.0-flash"
]
MAX_HISTORY = 12                    # —Å–∫–æ–ª—å–∫–æ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç —Ö—Ä–∞–Ω–∏–º
current_key_idx = 0
current_model_idx = 0
# –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∫–∞–∂–¥—ã–µ 4 —á–∞—Å–∞)
available_models: List[str] = MODELS.copy()
last_model_check_ts: float = 0.0

# ---------- –õ–æ–≥–∏ ----------
logging.basicConfig(
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    level=logging.INFO
)
log = logging.getLogger("wizardbot")

# ---------- –ö–æ–Ω—Ñ–∏–≥ –Ω–∞ —á–∞—Ç ----------
@dataclass
class ChatConfig:
    autopost_enabled: bool = False
    interval: int = 14400          # 4 —á
    min_messages: int = 10
    msg_size: str = "medium"      # small/medium/large
    last_post_ts: float = 0.0
    new_msg_counter: int = 0

# chat_id -> ChatConfig
configs: Dict[int, ChatConfig] = {}
# chat_id -> history (–¥–ª—è LLM)
history: Dict[int, List[Dict[str, str]]] = {}


# ---------- –í—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ----------
def sanitize_telegram_markdown(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç Markdown –æ—Ç –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö Telegram —ç–ª–µ–º–µ–Ω—Ç–æ–≤."""
    # –ó–∞–º–µ–Ω—è–µ–º –∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –¥–≤–æ–π–Ω—ã–º–∏ ** –Ω–∞ –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ *
    text = re.sub(r'\*\*(.*?)\*\*', r'*\1*', text)
    # –£–¥–∞–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (#, ##, ...)
    text = re.sub(r'^\s*#+\s+', '', text, flags=re.MULTILINE)
    # –£–¥–∞–ª—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏ (---, ***, ___), —Ç.–∫. –æ–Ω–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è
    text = re.sub(r'^\s*[-*=_]{3,}\s*$', '', text, flags=re.MULTILINE)
    return text

def get_cfg(chat_id: int) -> ChatConfig:
    if chat_id not in configs:
        configs[chat_id] = ChatConfig()
    return configs[chat_id]

def llm_request(chat_id: int, prompt: str, image: Optional[Image.Image] = None) -> Tuple[str, str]:
    global current_key_idx, current_model_idx

    chat_history = history.get(chat_id, [])
    models_to_try = available_models if available_models else MODELS

    for model_idx_offset in range(len(models_to_try)):
        model_idx = (current_model_idx + model_idx_offset) % len(models_to_try)
        model_name = models_to_try[model_idx]

        model_failed_on_all_keys = True
        for key_try in range(len(API_KEYS)):
            key_idx = (current_key_idx + key_try) % len(API_KEYS)
            api_key = API_KEYS[key_idx]

            try:
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel(model_name)
                
                # –ú–æ–¥–µ–ª–∏ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π, —Ç–∞–∫ —á—Ç–æ –¥–ª—è –Ω–∏—Ö –Ω–∞—á–∏–Ω–∞–µ–º —á–∞—Ç –∑–∞–Ω–æ–≤–æ
                current_chat_history = chat_history if not image else []
                chat_session = model.start_chat(history=current_chat_history)
                
                content_to_send = [prompt]
                if image:
                    content_to_send.insert(0, image)

                response = chat_session.send_message(content_to_send)
                answer = response.text
                
                # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º
                if not image:
                    history[chat_id] = chat_session.history

                current_key_idx = key_idx
                current_model_idx = model_idx
                model_failed_on_all_keys = False
                return (answer, model_name)
            except Exception as e:
                error_msg = str(e).lower()
                if any(phrase in error_msg for phrase in ["resource exhausted", "quota exceeded", "rate limit", "exceeded", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ", "limit"]):
                    log.info(f"Rate limit: key {key_idx+1}, model {model_name}, trying next key...")
                    continue
                else:
                    log.warning(f"Request failed: key {key_idx+1}, model {model_name}: {e}")
                    continue
        if model_failed_on_all_keys:
            log.info(f"Model {model_name} failed on all keys, trying next model...")
            continue
    raise Exception("All API keys/models failed")

def check_available_models() -> List[str]:
    global available_models, last_model_check_ts
    log.info("Checking available models...")
    working_models = []
    for model_name in MODELS:
        model_works = False
        for api_key in API_KEYS:
            try:
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel(model_name)
                response = model.generate_content("hi")
                # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –æ—Ç–≤–µ—Ç –≤–∞–ª–∏–¥–Ω—ã–π
                _ = response.text
                model_works = True
                break
            except Exception as e:
                error_msg = str(e).lower()
                if "not found" in error_msg or "invalid model" in error_msg or "does not exist" in error_msg:
                    log.warning(f"Model {model_name} not found or invalid.")
                    break 
                continue
        if model_works:
            working_models.append(model_name)
            log.info(f"Model {model_name} is available")
    
    if working_models:
        available_models = working_models
        last_model_check_ts = time.time()
        log.info(f"Available models updated: {working_models}")
    else:
        log.warning("No models available, using fallback list")
        available_models = MODELS.copy()
    return available_models

async def is_admin(update: Update, context: ContextTypes.DEFAULT_TYPE) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∞–≤—Ç–æ—Ä –∞–¥–º–∏–Ω–æ–º –≤ –≥—Ä—É–ø–ø–µ/—Å—É–ø–µ—Ä–≥—Ä—É–ø–ø–µ."""
    if update.effective_chat.type == "private":
        return True  # –≤ –ª–∏—á–∫–µ –≤—Å–µ –ø—Ä–∞–≤–∞
    member = await context.bot.get_chat_member(
        update.effective_chat.id, update.effective_user.id
    )
    return member.status in ("administrator", "creator")

def answer_size_prompt(size: str) -> str:
    mapping = {
        "small":   "–ö—Ä–∞—Ç–∫–æ:",
        "medium":  "–û—Ç–≤–µ—Ç—å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ:",
        "large":   "–û—Ç–≤–µ—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ, —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∫–æ–¥–∞ –∏ –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏:"
    }
    return mapping.get(size, "")

def split_long_message(text: str, max_length: int = 4096) -> List[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ max_length —Å–∏–º–≤–æ–ª–æ–≤."""
    if len(text) <= max_length:
        return [text]
    parts = []
    current = ""
    for line in text.split("\n"):
        if len(current) + len(line) + 1 <= max_length:
            current += (line + "\n" if current else line)
        else:
            if current:
                parts.append(current.strip())
            current = line
    if current:
        parts.append(current.strip())
    return parts

# ---------- –ö–æ–º–∞–Ω–¥—ã ----------
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üëã –Ø Gemini 2.5 –±–æ—Ç. /help ‚Äì —Å–ø—Ä–∞–≤–∫–∞\n\n"
        "‚ö†Ô∏è <b>–í–∞–∂–Ω–æ:</b> –í–∞—à–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ Google Gemini API. "
        "–ò—Å–ø–æ–ª—å–∑—É—è –±–æ—Ç–∞, –≤—ã —Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –¥–∞–Ω–Ω—ã—Ö –≤ Google –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.",
        parse_mode="HTML"
    )

async def help_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "<b>–ö–æ–º–∞–Ω–¥—ã:</b>\n"
        "/settings ‚Äì –ø–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n"
        "/autopost on|off ‚Äì –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ–ø–æ—Å—Ç—ã\n"
        "/set_interval &lt;—Å–µ–∫&gt; ‚Äì –∏–Ω—Ç–µ—Ä–≤–∞–ª –∞–≤—Ç–æ–ø–æ—Å—Ç–∞\n"
        "/set_minmsgs &lt;n&gt; ‚Äì –º–∏–Ω–∏–º—É–º –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –ø–µ—Ä–µ–¥ –∞–≤—Ç–æ–ø–æ—Å—Ç–æ–º\n"
        "/set_msgsize &lt;small|medium|large&gt; ‚Äì —Ä–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–æ–≤ –±–æ—Ç–∞\n"
        "/reset ‚Äì –æ—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞\n\n"
        "‚ö†Ô∏è <b>–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å:</b> –í–∞—à–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ Google Gemini API –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. "
        "–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ —Ö—Ä–∞–Ω–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –ø–∞–º—è—Ç–∏ –∏ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ.",
        parse_mode="HTML"
    )

async def settings_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    cfg = get_cfg(update.effective_chat.id)
    txt = (
        f"–ê–≤—Ç–æ–ø–æ—Å—Ç—ã: {'–≤–∫–ª—é—á–µ–Ω—ã' if cfg.autopost_enabled else '–≤—ã–∫–ª—é—á–µ–Ω—ã'}.\n"
        f"–ò–Ω—Ç–µ—Ä–≤–∞–ª –∞–≤—Ç–æ–ø–æ—Å—Ç–∞: {cfg.interval//3600} —á, "
        f"–º–∏–Ω–∏–º—É–º –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {cfg.min_messages}.\n"
        f"–†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–æ–≤: {cfg.msg_size}."
    )
    await update.message.reply_text(txt)

async def reset(update: Update, context: ContextTypes.DEFAULT_TYPE):
    history.pop(update.effective_chat.id, None)
    await update.message.reply_text("–ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞ ‚úÖ")

async def autopost_switch(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await is_admin(update, context):
        return
    args = (context.args or [])[:1]
    if not args or args[0] not in {"on", "off"}:
        await update.message.reply_text("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /autopost on|off")
        return
    cfg = get_cfg(update.effective_chat.id)
    cfg.autopost_enabled = args[0] == "on"
    await update.message.reply_text(f"–ê–≤—Ç–æ–ø–æ—Å—Ç—ã {'–≤–∫–ª—é—á–µ–Ω—ã' if cfg.autopost_enabled else '–≤—ã–∫–ª—é—á–µ–Ω—ã'}")

async def set_interval(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await is_admin(update, context): return
    try:
        sec = int(context.args[0])
        cfg = get_cfg(update.effective_chat.id)
        cfg.interval = max(300, sec)
        await update.message.reply_text(f"–ò–Ω—Ç–µ—Ä–≤–∞–ª –∞–≤—Ç–æ–ø–æ—Å—Ç–∞ = {cfg.interval} —Å–µ–∫")
    except (IndexError, ValueError):
        await update.message.reply_text("–ü—Ä–∏–º–µ—Ä: /set_interval 7200")

async def set_minmsgs(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await is_admin(update, context): return
    try:
        n = int(context.args[0])
        cfg = get_cfg(update.effective_chat.id)
        cfg.min_messages = max(1, n)
        await update.message.reply_text(f"–ú–∏–Ω–∏–º—É–º —Å–æ–æ–±—â–µ–Ω–∏–π = {cfg.min_messages}")
    except (IndexError, ValueError):
        await update.message.reply_text("–ü—Ä–∏–º–µ—Ä: /set_minmsgs 10")

async def set_msgsize(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await is_admin(update, context): return
    size = (context.args or [""])[0].lower()
    if size not in {"small", "medium", "large"}:
        await update.message.reply_text("–í–∞—Ä–∏–∞–Ω—Ç—ã: small | medium | large")
        return
    cfg = get_cfg(update.effective_chat.id)
    cfg.msg_size = size
    await update.message.reply_text(f"–†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–æ–≤ = {size}")

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–∞ ----------
async def handle_msg(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message or not update.message.text:
        return
    chat_id = update.effective_chat.id
    text = update.message.text

    # —Å—á—ë—Ç—á–∏–∫ –¥–ª—è –∞–≤—Ç–æ–ø–æ—Å—Ç–∞
    cfg = get_cfg(chat_id)
    cfg.new_msg_counter += 1

    # LLM prompt
    sys_prompt = answer_size_prompt(cfg.msg_size)
    prompt = f"{sys_prompt}\n{text}" if sys_prompt else text

    await context.bot.send_chat_action(chat_id, "typing")
    loop = asyncio.get_event_loop()
    try:
        reply, model_used = await loop.run_in_executor(None, llm_request, chat_id, prompt)
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º
        model_display = model_used.replace("gemini-", "").replace("-", " ").title()
        full_reply = f"ü§ñ {model_display}\n\n{reply}"
    except Exception as e:
        log.exception(e)
        full_reply = "‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏."

    # –û—á–∏—â–∞–µ–º –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    sanitized_reply = sanitize_telegram_markdown(full_reply)
    message_parts = split_long_message(sanitized_reply)
    for i, part in enumerate(message_parts):
        try:
            await update.message.reply_text(
                part, disable_web_page_preview=True, parse_mode="Markdown"
            )
        except BadRequest as e:
            if "entities" in str(e).lower() or "parse" in str(e).lower():
                log.warning("Markdown parse failed, sending plain text. Error: %s", e)
                await update.message.reply_text(part, disable_web_page_preview=True)
            elif "too long" in str(e).lower():
                # –ï—Å–ª–∏ –¥–∞–∂–µ –±–µ–∑ Markdown —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ—â—ë –±–æ–ª—å—à–µ
                plain_parts = split_long_message(part, max_length=4000)
                for plain_part in plain_parts:
                    await update.message.reply_text(plain_part, disable_web_page_preview=True)
            else:
                log.error("Failed to send message: %s", e)


async def handle_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message:
        return
    chat_id = update.effective_chat.id
    text = update.message.caption or "–û–ø–∏—à–∏ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
    
    # —Å—á—ë—Ç—á–∏–∫ –¥–ª—è –∞–≤—Ç–æ–ø–æ—Å—Ç–∞
    cfg = get_cfg(chat_id)
    cfg.new_msg_counter += 1
    
    # –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ç–æ - –±–µ—Ä–µ–º —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ
    photos = update.message.photo
    if not photos:
        return
    photo = photos[-1]
    
    await context.bot.send_chat_action(chat_id, "typing")
    
    # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–æ—Ç–æ
    file = await context.bot.get_file(photo.file_id)
    photo_bytes = await file.download_as_bytearray()
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ PIL Image
    image = Image.open(io.BytesIO(photo_bytes))
    
    # LLM prompt
    sys_prompt = answer_size_prompt(cfg.msg_size)
    prompt = f"{sys_prompt}\n{text}" if sys_prompt else text
    
    loop = asyncio.get_event_loop()
    try:
        reply, model_used = await loop.run_in_executor(None, llm_request, chat_id, prompt, image)
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º
        model_display = model_used.replace("gemini-", "").replace("-", " ").title()
        full_reply = f"ü§ñ {model_display}\n\n{reply}"
    except Exception as e:
        log.exception(e)
        full_reply = "‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏."
    
    # –û—á–∏—â–∞–µ–º –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    sanitized_reply = sanitize_telegram_markdown(full_reply)
    message_parts = split_long_message(sanitized_reply)
    for i, part in enumerate(message_parts):
        try:
            await update.message.reply_text(
                part, disable_web_page_preview=True, parse_mode="Markdown"
            )
        except BadRequest as e:
            if "entities" in str(e).lower() or "parse" in str(e).lower():
                log.warning("Markdown parse failed, sending plain text. Error: %s", e)
                await update.message.reply_text(part, disable_web_page_preview=True)
            elif "too long" in str(e).lower():
                plain_parts = split_long_message(part, max_length=4000)
                for plain_part in plain_parts:
                    await update.message.reply_text(plain_part, disable_web_page_preview=True)
            else:
                log.error("Failed to send message: %s", e)

# ---------- JOB –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π ----------
async def check_models_job(context: CallbackContext):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∂–¥—ã–µ 4 —á–∞—Å–∞ (14400 —Å–µ–∫)"""
    log.info("Checking available models...")
    loop = asyncio.get_event_loop()
    try:
        await loop.run_in_executor(None, check_available_models)
    except Exception as e:
        log.exception(f"Error checking models: {e}")

# ---------- JOB –¥–ª—è –∞–≤—Ç–æ–ø–æ—Å—Ç–æ–≤ ----------
async def autopost_job(context: CallbackContext):
    for chat_id, cfg in list(configs.items()):
        if not cfg.autopost_enabled:
            continue
        if cfg.new_msg_counter < cfg.min_messages:
            continue
        if time.time() - cfg.last_post_ts < cfg.interval:
            continue

        prompt = (
            f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö {cfg.new_msg_counter} —Å–æ–æ–±—â–µ–Ω–∏–π "
            "–∏–∑ –≥—Ä—É–ø–ø–æ–≤–æ–≥–æ —á–∞—Ç–∞. –í—ã–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –∏–¥–µ–∏."
        )
        log.info(f"Autopost in chat {chat_id}")
        try:
            loop = asyncio.get_event_loop()
            summary, model_used = await loop.run_in_executor(None, llm_request, chat_id, prompt)
            model_display = model_used.replace("gemini-", "").replace("-", " ").title()
            
            message_text = f"üì∞ –ê–≤—Ç–æ–¥–∞–π–¥–∂–µ—Å—Ç ({model_display}):\n{summary}"
            sanitized_text = sanitize_telegram_markdown(message_text)
            message_parts = split_long_message(sanitized_text)
            for part in message_parts:
                try:
                    await context.bot.send_message(chat_id, part, parse_mode="Markdown")
                except BadRequest as e:
                    if "entities" in str(e).lower() or "parse" in str(e).lower():
                        log.warning("Markdown parse failed for autopost, sending plain text. Error: %s", e)
                        await context.bot.send_message(chat_id, part)
                    elif "too long" in str(e).lower():
                        plain_parts = split_long_message(part, max_length=4000)
                        for plain_part in plain_parts:
                            await context.bot.send_message(chat_id, plain_part)
                    else:
                        log.error("Failed to send autopost message: %s", e)

        except Exception as e:
            log.error(f"Autopost failed for chat {chat_id}: {e}")

# ---------- MAIN ----------
def main():
    token = os.getenv("TG_TOKEN")
    if not token:
        raise RuntimeError("TG_TOKEN env not set")

    app = ApplicationBuilder().token(token).build()

    # regular commands
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("help", help_cmd))
    app.add_handler(CommandHandler("settings", settings_cmd))
    app.add_handler(CommandHandler("reset", reset))

    # admin commands
    app.add_handler(CommandHandler("autopost", autopost_switch))
    app.add_handler(CommandHandler("set_interval", set_interval))
    app.add_handler(CommandHandler("set_minmsgs", set_minmsgs))
    app.add_handler(CommandHandler("set_msgsize", set_msgsize))

    # messages
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_msg))
    app.add_handler(MessageHandler(filters.PHOTO, handle_photo))

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º job_queue –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    if app.job_queue:
        # –ü–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ 60 —Å–µ–∫—É–Ω–¥ –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–∞
        app.job_queue.run_repeating(check_models_job, interval=14400, first=60)
        app.job_queue.run_repeating(autopost_job, interval=60, first=60)
        log.info("JobQueue initialized")
    else:
        log.warning("JobQueue not available - scheduled jobs disabled")

    log.info("Bot started üöÄ")

    # --- –ó–∞–ø—É—Å–∫–∞–µ–º Flask-—Å–µ—Ä–≤–µ—Ä –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ ---
    # Render –ø–µ—Ä–µ–¥–∞–µ—Ç –ø–æ—Ä—Ç —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è PORT
    port = int(os.environ.get("PORT", 8080))
    flask_thread = threading.Thread(target=lambda: flask_app.run(host='0.0.0.0', port=port, use_reloader=False))
    flask_thread.daemon = True # –ü–æ—Ç–æ–∫ –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è, –∫–æ–≥–¥–∞ –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å
    flask_thread.start()
    log.info(f"Flask app started on port {port}")
    # ---------------------------------------------------

    app.run_polling(allowed_updates=Update.ALL_TYPES, stop_signals=None)


if __name__ == "__main__":
    main()